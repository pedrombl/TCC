\section{Scalability}

\subsection{Definition of Scalability}
In a large scale environment, a choreography must be able to attend different scenario sizes. Web Services Choreographies will probably have some periods that will require more resources that it is usual to maintain its performance.  Suppose that a specific choreography, for example, receives usually N requests per second but, in some specific times, receives 10N requests. It must have tolerance to this increase of access quickly.

To support this issue, we may increase its system capability between this period of high demand. We want to have the same performance that we had previously. Therefore, we must know how much of resources we should increase to achieve this goal.  Each choreography will have its particular solution. This will depends on how the choreography is implemented. Bad choreographies architectures will need to increase a lot to achieve that, someones won't even achieve that.

For instance, in the example above, we could increase the resources in the same scale that the number of access increased to gain the same performance. However, only that will not assure we will perform that, i.e.  the choreography may have a bottleneck that will not disappear if we add more resources. Thus, we also must pay attention with the implementation of the choreography. We gain scalability with the collaboration of the hardware and the software. 

Architectures that achieved the same performance by increasing the architecture capability with the same proportion as the increase of the problem size are scalable [\citet{QUINN}]. For \citet{LAW}, a simulation is scalable if the improvements in simulation capability express in direct proportion improvements in architectural capability. However, these definitions just say if an application is scalable or not. Consequently, we can not compare the scalability of two applications by saying that one is more scalable than the other. To conclude how scalable an application is, we need to quantify it. Law quantify simulation scalability in a mathematical model, based on two capabilities, simulation and architectural. 

\subsection{Quantifying scalability}

A simulation is built on a set of values that describe its state, this is called measures of interest. We also know it must meet a set of performance requirements. A simulation attain acceptable performance if it execute with correctness and satisfy the performance requirement. For instance, in a web service stress simulation, the number of requests per second is a measure of interest, and a performance requirement is the average response time. If the simulation do not have a minimum satisfactory response time average with correct responses, it did not achieve acceptable performance. 

Based on the measures of interested and the performance requirements, we can tell the complexity of the problem. The author define the complexity size with the function $S(n_{1}..n_{s})$, where $n_{1}$ to $n_{s}$ is a set of $s$ variables. This variables are measures of interested and performance requirements inputs. Its assumed that if one of this variables increase, the simulation size function will increase or at least stay the same, but never decrease. There are different ways to define the complexity of a problem. It depends on the characteristics we want to analyze.

We've seen performance as a requirement, however we may also be interested in its precise values. For example, the memory usage may be a metric that we want to minimize even if we have not defined what is the maximum value that it can achieve. Thus, Law defines a function that characterize the performance metrics, $M(m_{1}..m_{q})$, where $m_{1}$ to $m_{q}$ is a set of $q$ performance metrics. If a performance metric increase, its assumed that the performance improves. We can reverse the metrics that do not behave this way. If $Y$ is the memory usage metric, we can pass $\frac{1}{Y}$ as input. With these definition we conclude that a simulation capability is the relation of the functions $S$ and $M$, defined as $C(S,M) = SM$.

As said previously, to measure scalability we need also to quantify the architectural capability. The inputs that we need to achieve this are the architecture characteristics, such as the CPU clock speed. The function $P(h_{1}..h_{p})$ describes it, where $h_{1}$ to $h_{p}$ are the variables that represent the measures of the architecture.

With the definition of these two functions, we choose a architecture with capability $P_{1}$. $C_{1}(S,M)$ will be choose as the largest value that the architecture of capability $P_{1}$ can compute the problem with correctness and meet all the performance requirements for every element of the simulation capability domain. Therefore, $k = \frac{C_{1}(S,M)}{P_{1}}$, is the best ratio between simulation capability and architectural capability that is achievable with the chosen system architecture.

Let $C_{i}$ and $P_{i}$ be capabilities $i$ times greater than $C_{1}$ and $P_{1}$, respectively. Therefore, the author defines scalability as the largest real-value $j$ such as $\frac{C_{i}(S,M)}{P_{i}} \geq k$ for every $i$ in the real-value interval $[1,j]$ and $P_{i}$ can compute the problem of complexity size $C_{i}(S,M)$ with correctness and meet all the performance requirements. With these definition we can are verify the interval where the ratio between the simulation capability and the architectural capability stays at least equal as it was initially. If $j = \infty$, we say that the system is fully scalable.

This definition, however, has some limitations as pointed out by Law. Its very difficult to define the architectural capability funciton. The scalability function presented above has a highly dependency on the definition of the architectural and simulation capabilities. This means that, the same system can have different ranges of scalability. 





















