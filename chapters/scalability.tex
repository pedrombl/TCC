\section{Scalability Testing}
\label{scalabilityTesting}

Currently on the Internet, we have high a variation of demand of the applications that run on it. Many of these applications cannot support this high variation properly because of bad implementations or bad architectures. These problems are related to the application scalability. Scalability testing aims at analyzing it and verifying which part of the systems are not scaling properly. First, we need to define what the term scalability means. Then, we must know how we analyze the application scalability to test it. 

\subsection{Definition of Scalability}
A choreography, in a large scale environment, must be able to cope with different scenario sizes. Suppose that a specific choreography usually receives $N$ requests per second but, in some specific times, receives $10N$ requests per second. It must tolerate this increase of access quickly.

To support this, we may increase the system capacity during this period of high demand. We want to have the same performance that we had previously. Therefore, we must know how many resources we should increase to achieve this goal.  Each choreography will have its particular solution. This will depend on how the choreography is implemented. Choreographies with a bad architecture will need to increase its resources heavily to achieve the same performance.

For instance, in the example above, we could increase the resources in the same scale that the number of access increased to gain the same performance. However, this will not work properly if the choreography is not well implemented, i.e.,  the choreography may have a bottleneck that will not disappear if we add more resources. Thus, we also must pay attention to the implementation of the choreography. We gain scalability with the collaboration of hardware and software. 

An application is scalable if it achieves the same performance when increasing the architecture capacity with the same proportion that the problem size increases~[\cite{QUINN}]. To \cite{LAW}, an application is scalable if the improvements in the execution capability express in direct proportion improvements in architectural capability. However, these definitions just say if an application is scalable or not. Consequently, we cannot compare the scalability of two applications by saying that one is more scalable than the other. To conclude how scalable an application is, we need to quantify it. Law quantifies simulation scalability in a mathematical model, based on two capabilities: execution and architectural. 

\subsection{Quantifying Scalability}
\label{quantifyingScalability}

The function that quantify the execution capability is based on two functions: the complexity size of the problem function, and the performance metric function. 

The complexity size of the problem is based on a set of values that describes its state, which are called measures of interest. For instance, in a web service stress simulation, the number of requests per second is a measure of interest. If the number of requests per second increases, the complexity size of the problem increases too. Law defines the complexity size with the function $S(n_{1}..n_{s})$, where $n_{1}$ to $n_{s}$ is a set of $s$ measures of interest. It is assumed that if one of these variables increase, the simulation size function will increase or at least stay the same, but will never decrease. There are different ways to define the complexity of a problem. It depends on the characteristics we want to analyze.

When we execute the application, a set of performance characteristics can be analyzed. Thus, we also have a function that defines the importance of the metric performances, $M(m_{1}..m_{q})$, where $m_{1}$ to $m_{q}$ is a set of $q$ performance metrics. If a performance metric increases, it is assumed that the performance decreases. We can reverse the metrics that do not behave this way. With this definition, we conclude that a simulation capability is the relation of the functions $S$ and $M$, defined as $C(S,M) = \frac{S}{M}$.

As said previously, to measure scalability we need also to quantify the architectural capability. The inputs that we need to define are the architecture characteristics, such as the CPU clock speed. The $P(h_{1}..h_{p})$ function describes it, where $h_{1}$ to $h_{p}$ are the variables that represent the measures of the architecture.

With the definition of these two functions, we choose an initial architectural capability $P_{1}$, initial measures of interest $S_{1}$, and the metric performances $M_{1}$ we want to analyze. After the execution, a constant $k = \frac{C_{1}(S_{1},M_{1})}{P_{1}}$ is calculated. Let $C_{i}$ and $P_{i}$ be capabilities $i$ times greater than $C_{1}$ and $P_{1}$, respectively. Therefore, Law defines scalability as the largest real-value $j$ such that $\frac{C_{i}(S_{i},M_{i})}{P_{i}} \geq k$ for every $i$ in the interval $[1,j]$. With this definition, we can are verify the interval where the ratio between the execution capability and the architectural capability stays at least equal as it was initially. If $j = \infty$, we say that the system is fully scalable.

This definition, however, has some limitations as pointed out by Law. One of them is the difficulty to define an architectural capability function that expresses the exact importance among each hardware characteristic. For example, a system with 512MB of RAM and a CPU of 100GHz may or may not have the same capability of one with 256MB and a CPU of 200GHz. Another one is the high dependency on the definition of the architectural and simulation capabilities in the scalability function. This means that, the same system can have different ranges of scalability depending on how we define these functions. The point here is that it is very difficult to quantify the P and C variables. Therefore, if we cannot quantify them properly, we will not be able to quantify the scalability of the system. 

These restrictions show us that scalability is something relative, a system may have different ranges of scalability depending on the importance that we give to its elements. However, despite these limitations, we can adapt it to something more usable. We can use the scalability function to compare two systems with the same architectural and simulation capabilities. We can also limit the number of the capability functions input to one. Let $s$ be the input of the complexity size of the problem, and $m$ be the input of the performance metric for simulation capability function $C_{i}$ and, $h$ for the architectural capability function. Thus, we can evaluate if a system is more scalable then another based on these three variables. Law presents the final definition of scalability as the largest real-value $j$, such that $\frac{C_{i}(s, m)}{P_{i}(h)} \geq k$ for every i in the real-value interval $[1.0, j]$.

With this description, the author narrows the scalability comparison of two systems, regarding to a specific aspect of the simulation and architecture. For example, we chose $s$ as the size of data sent per request to a web service, m as the percentage of used memory, and h as the total memory of the web service hardware. Then, we can compare if a system has more scalability than the other in terms of space utilization.

\subsection{Quantifying Scalability in Practice}

The definition above, made us conclude that the function $\frac{C_{i}(S_{i},M_{i})}{P_{i}} = \frac{S_{i}}{M_{i} \times P_{i}}$ should be constant after each execution. Therefore, $\frac{S_{i}}{M_{i}}$ should vary in the same proportion as $P_{i}$. Since the execution capability is based on two functions, $S$ as the complexity size of the problem and $M$ as the performance metric, we can increase it in different ways:
\begin{enumerate}
\item Increasing S with the same proportion as the architecture capability. In this case, we expect M to be constant.
\item Fixing S. Then, we expect that M will decrease in the same proportion as the architecture capability.
\item Increasing S in a different proportion of the architecture capability. Then, we expect that M will decrease in a proportion that will make the $\frac{S}{M}$ function increase in the same proportion as the architecture capability.
\end{enumerate}

If we use the first option, the performance metric of an application that has a good scalability should stay constant and one that has a bad scalability will increase. Thus, an application with good scalability will have a performance metric graph with a horizontal line (Figure \ref{fig:goodscalability}) and an application with bad scalability will have an increasing line (Figure \ref{fig:badscalability}).

\begin{figure}[ht]
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[scale=0.7]{images/scalabilitygood}
\caption{Performance metric graph with \\* a \textbf{good scalability}}
\label{fig:goodscalability}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[scale=0.7]{images/scalabilitybad}
\caption{Performance metric graph with \\* a \textbf{bad scalability}}
\label{fig:badscalability}
\end{minipage}
\end{figure}


In this project, we will use the first option because we have control over the variables that define the complexity of the problem. Also, it is easier to visualize, in a performance metric graph, the difference between an application with good scalability and bad scalability.

\subsection{Types of Scalability}
With the definition presented in the Section \ref{quantifyingScalability}, we conclude that there are different ways to examine the scalability of a system. \cite{BONDI} presents four types of scalability that we can observe:

\begin{description}
\item[Load scalability.] A system has more load scalability if it behaves properly with light, moderate, or heavy loads. In this case, the word \emph{properly} is related to functional and non-functional aspects, such as resource contention, excess of delay, and ineffective memory consumption. Thus, to measure the load scalability of a system using the scalability function, one would use, for instance,  number of request per second as the input for the simulation capability function, the number of processors as the input for the architectural capability, and fixing the response time average as a performance metric.
\item[Space scalability.]  A system has a bad space scalability if its memory requirements grow largely when the number of supported items increases. For example, measuring the scalability based on the data size sent per request as an input for the simulation capability function, the RAM of the system hardware, and fixing the percentage of RAM used as a performance metric.
\item[Space-time scalability.] A system has more space-time scalability if it works properly, without large delays, when the number of its objects increases. If a search system takes more time to find an information because of the increasing of data storage, even if its hardware resources have grown too, we conclude that it does not have space-time scalability.
\item[Structural scalability.] A system is structurally scalable if it does not prevent the increasing of objects that it encompasses. For example, if a peer-to-peer system has a maximum number of nodes that it can support and another has not, the latter is more structural scalable than the former.
\end{description}

These types of scalability give us a direction of what we need to observe when comparing the scalability of the systems. It is important to note that they are, sometimes, dependent from each other. A system that needs space-time scalability may also need space scalability because a huge growth of memory might cause a delay to find information.

\subsection{Related Work}

\cite{STAS} present the Scalability Testing and Analysis System (STAS), a tool that supports the users in running scalability analysis of algorithms and systems. STAS analyzes scalability using the metric \emph{isospeed-efficiency scalability}, also known as \emph{isospeed-e}. 

The architecture capability used in this metric is the sum of the supported speeds of all system nodes. Let $h_{i}$ be the supported speed of a specific node $i$, then the architecture capability of the system is $P(n) = \sum_{i=1}^{n} h_i$ , where $n$ is the number of nodes. 

A function that characterizes the complexity of the problem, defined by the user, combined with the execution time, which is the performance metric, results in the execution capability function $C = \frac{S}{T}$, where $S$ is the value that represents the complexity of the problem and $T$ is the execution time metric. Therefore the scalability function is defined as $\frac{C}{P}$.

STAS is composed of four components as shown in Figure \ref{stasarchitecture}: system characterization component, algorithm pre-analysis component, scalability tester component, and scalability analyzer component.

\begin{figure}[htbp]
\begin{center}
	\includegraphics[scale=0.6]{images/stasarchitecture}
\caption{STAS Architecture [\cite{STAS}]}
\label{stasarchitecture}
\end{center}
\end{figure}

The system characterization component is responsible for calculating the supported speed of each available node by measuring its speed in FLOPS. The algorithm pre-analysis component is responsible for calculating the problem complexity defined by the user. 

The scalability tester component executes the application and calculates the execution time.  It calculates $C_1$ by applying the function described above with the obtained values. After that, it executes the application doubling the number of nodes and the parameter of the complexity problem function. $C_2$ is obtained with the execution information. This process is done until it reaches the maximum number of nodes available. The results are compared by the scalability analyzer component that concludes how scalable the application is.

STAS supports only parallel applications, such as MPI, PVM, and HPF. Thus, it is not applicable for SOA applications, where the implementation of the services that integrate the system are heterogeneous. It restricts the choice of the scalability function variables. The tester, for instance, cannot test the scalaiblity based on the number of requests per second, since the complexity of the problem is only defined based on the complexity of the application algorithm. Our Scalability Testing Framework, described in the next section, aims at providing the freedom to choose the scalability test type desired.

\cite{p2p} presents a methodology and a framework for testing peer-to-peer (P2P) applications. The authors combine functional and non-functional tests because P2P applications are volatile and have variations in its scale, which might affect the functionalities of the system. The framework is able to manipulate all nodes of the application and test it in different scenarios. It applies scalability testing by changing the scales of the application and validating their functionalities. However, each execution scenario must be specified, which can make the test specification cumbersome.

By using Java Annotations, the developer specifies the behavior and the tests of the nodes. These annotations give to the developer the freedom to manipulate and test the application in different scenarios, and scales.










